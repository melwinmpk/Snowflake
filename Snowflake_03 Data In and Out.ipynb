{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666b80ae",
   "metadata": {},
   "source": [
    "<h2>Snowflake suppors data loading in 2 Primary Ways</h2>\n",
    "<ul>\n",
    "    <li>Copy Command</li>\n",
    "    <li>Snowpipe</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb0e2b",
   "metadata": {},
   "source": [
    "<h3>Copy Command</h3>\n",
    "<ul>\n",
    "<li>The copy command requires the usage of a virtual warehouse.</li>\n",
    "<li>When new data is loaded into a table, one or more new micro partitions are written to the storage,</li>\n",
    "<li>and every time a new micro partition for a table is written, the table's metadata is changed.</li>\n",
    "<li>This metadata contains information on the micro partitions, the range of values for each columns and other optimization information.</li>\n",
    "<li>At the same time, extra metadata about the file that has been imported into the database is stored</li>\n",
    "<li>in the metadata database.</li>\n",
    "<li>And all of this metadata is stored in Snowflake's cloud services layer.</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dcb4a0",
   "metadata": {},
   "source": [
    "<h3>Snowpipe</h3>\n",
    "<ul>\n",
    "<li>So Snowpipe is the ideal technique for loading data when the data is arriving continuously in a messaging or a streaming manner.</li>\n",
    "<li>So regardless of how the data is loaded into Snowflake, new micro partitions are created and metadata updates are performed.</li>\n",
    "    \n",
    "<li>Continuous loading of data</li>\n",
    "<li>Data is loaded within minutes of arriving in a stage</li>\n",
    "<li>Serverless</li>\n",
    "<li>Managed by Snowflake</li>\n",
    "    <li>Scaled up and scaled down automatically as needed</li>\n",
    "    <li>Does not use user-managed virtual warehouses</li>\n",
    "    <li>Billed seperately from the virtual warehouse costs</li>\n",
    "    \n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d571d",
   "metadata": {},
   "source": [
    "<h3>Before data can be processed into a snowflake table, it must first be made available in a snowflake stage.</h3>\n",
    "<ul>\n",
    "    <li>Stages in snowflake help snowflake load and unload data easily.Similar to how data warehouse is used.</li>\n",
    "    <li>Staging Snowflake uses a stage object.</li>\n",
    "    <li>Snowflake uses stages to aid in the loading and unloading of data.</li>\n",
    "    <li>In order to load data into a snowflake table.</li>\n",
    "    <li>The data first must be available in a snowflake stage.</li>\n",
    "    <li>Copy command then can be used to load data into a table after it was made available in a stage.</li>\n",
    "    <li>Similarly, data unloading or exporting is also performed via stages.</li>\n",
    "<!--     <li>This allows snowflake access to the data so that it can be loaded into a table.</li>\n",
    "    <li>Once the data is available in a stage, the copy command can then be used to copy the data into a database table.</li> -->\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64416b06",
   "metadata": {},
   "source": [
    "<h2>Types of Stage</h2>\n",
    "<ul>\n",
    "    <li>External Stages</li>\n",
    "    <li>Internal Stages</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Internal Stages</h3>\n",
    "<ul>\n",
    "    <li>Internal stages store data in themselves.</li>\n",
    "    <li>Once data has been uploaded into an internal stage, it can then be loaded into a table using the copy command.</li>\n",
    "    <li>There are three types of internal stages, so </li>\n",
    "    <li><b>Named internal stages</b></li> \n",
    "    <ul>\n",
    "    <li>The stage objects that can be created, dropped and modified as required.</li>\n",
    "    <li>They offer a lot more flexibility than user and table stages.</li>\n",
    "    <li>They have various advantages over the user and table stages, so they allow you to customize the file format parameter.</li>\n",
    "        <li>That means you can set up a stage once and use it to load several files and several tables.</li>\n",
    "        <li>they can be assigned security and user access rights and therefore they can be shared across multiple users.</li>\n",
    "    </ul>    \n",
    "    <li><b>Table Stage</b></li>\n",
    "    <ul>\n",
    "        <li>it is automatically created for each table and can be used to load data into that table.</li>\n",
    "        <li>Files that are loaded into a table stage then can be loaded into the table associated with that stage.</li>\n",
    "        <li>Multiple users can access a table stage, but it can only load data into one table.</li>\n",
    "        <li>And that one table is the table that is associated with the stage.</li>\n",
    "        <li>Table stages do not allow you to change the file format options, so these options must be included as part of the copy command.</li>\n",
    "        <li>Table stage is also do not allow data transformations to be applied while loading data.</li>\n",
    "    </ul>    \n",
    "    <li><b>User Stage</b></li>\n",
    "    <ul>\n",
    "        <li>each user gets a personal stage as well, which is created as soon as the user is created. User stages are also a subtype of an internal stage.</li>\n",
    "        <li>Users cannot access each other's stages, so a user can access only their own stage.</li>\n",
    "        <li>They cnnot be modified</li>\n",
    "    </ul>\n",
    "    <li>Data from on premises system can be loaded into snowflake using internal stages.</li>\n",
    "    <li>So each table and user in Snowflake are assigned an internal stage by default, and it is not possible to drop or modify that stage.</li>\n",
    "    <li>A very important point that data stored in a snowflake internal stage is counted towards your snowflake account. Total storage.So it is suggested that if you have processed data through internal stages and that data has been loaded\n",
    "into tables, it is advised that the internal stages are cleared so that you can avoid unnecessary costs.</li>\n",
    "    \n",
    "</ul>    \n",
    "\n",
    "Example of Internal Stage\n",
    "\n",
    "<pre>\n",
    "# NAMED INTERNAL STAGE\n",
    "CREATE OR REPLACE STAGE LU_Airport_CSV_Stage\n",
    "FILE_FORMAT = (TYPE = csv FIELD_DELIMITER = '.' FIELD_OPTIONALLY_ENCLOSED_BY = '\"' SKIP_HEADER = 1);\n",
    "</pre>\n",
    "\n",
    "<img width=\"803\" alt=\"image\" src=\"https://github.com/melwinmpk/AmazonBooks_DataPipeline/assets/25386607/f6c70378-5c33-4937-b511-cd2280d8d560\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11aee0",
   "metadata": {},
   "source": [
    "<h3>External Stages</h3>\n",
    "<ul>\n",
    "<li>External stage refers to a storage location that is outside of Snowflake example AWS S3, Azure Blob storage, Google Storage </li>\n",
    "<li>Once an external stage has been defined, the copy command then can be used to load data from that external\n",
    "stage into a snowflake table.</li>\n",
    "    <li>The stage itself does not store anything, but rather refers to the cloud storage.</li>\n",
    "    <li>Snowflake does not control the external storage cloud location.</li>\n",
    "    <li>In External stage definitions. They include information on how to connect to the cloud storage location. This information covers the path to the cloud storage credentials to securely access the cloud storage. And if the data in cloud storage has been encrypted, the key for decrypting this data and then finally the format of the file being loaded.</li>\n",
    "    <li>Snowflake does not charge for the fees associated with storing data in an external stage because an external stage refers to a cloud storage location which is outside of Snowflake's network.The fees of storing data in an external cloud storage therefore is charged by the cloud provider who\n",
    "\n",
    "actually are providing that external storage.</li>\n",
    "</ul>\n",
    "Example of a External table\n",
    "<pre>\n",
    "CREATE OR REPLACE STAGE my_first_external_stage\n",
    "url='s3://<bucket_name>/folder/subfolder'\n",
    "storage_integration = < storage_integration_name >\n",
    "file_format = (type = 'CSV' field_delimiter = ',');\n",
    "</pre>\n",
    "<img width=\"860\" alt=\"image\" src=\"https://github.com/melwinmpk/AmazonBooks_DataPipeline/assets/25386607/484fcddf-1330-4145-a678-7e703f84f916\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519db4e",
   "metadata": {},
   "source": [
    "<h3>Hands on On Premesis System </h3>\n",
    "\n",
    "link=> https://ca-flight-data-for-download.s3.amazonaws.com/L_AIRPORT_ID.csv\n",
    "\n",
    "please refer \"Assignment 4 Lab Objective Load data into a table using named internal stage\" for further understanding\n",
    "<p>Loading the Data that is sotred in csv format in PC to Snowflake </p>\n",
    "\n",
    "<!-- <pre>\n",
    "\n",
    "CREATE TABLE LU_Airport(\n",
    "Airline_ID Number,\n",
    "Airline_Description String\n",
    ");\n",
    "\n",
    "# NAMED INTERNAL STAGE\n",
    "CREATE OR REPLACE STAGE LU_Airport_CSV_Stage\n",
    "FILE_FORMAT = (TYPE = csv FIELD_DELIMITER = '.' FIELD_OPTIONALLY_ENCLOSED_BY = '\"' SKIP_HEADER = 1);\n",
    "\n",
    "USE DATABASE dev_lnd;\n",
    "\n",
    "PUT 'file:///C:/snowpro/..../L_AIRPORT_ID.csv' @LU_Airport_CSV_Stage;\n",
    "\n",
    "LIST @LU_Airport_CSV_Stage;\n",
    "\n",
    "USE DATABASE dev_lnd;\n",
    "\n",
    "COPY INTO LU_Airport FROM @LU_Airport_CSV_Stage;\n",
    "\n",
    "SELECT * FROM LU_Airport;\n",
    "\n",
    "REMOVE @LU_Airport_CSV_Stage;\n",
    "\n",
    "</pre> -->\n",
    "\n",
    "<b>Is the data encrypted prior to being transferred to a Snowflake internal stage?</b><br>\n",
    "Yes. The Data is encrypted prior before being transferred to a Snowflake Internal stage <br>\n",
    "<b>Is the data in a Snowflake internal stage stored in an encrypted format?</b><br>\n",
    "Yes. By default the data that is stored in Internal stage is encrypted. (in Gzip format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321fda8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b0d3f4",
   "metadata": {},
   "source": [
    "<p>to connect to snowsql</p>\n",
    "<ul>\n",
    "<li>install snowsql in the PC</li>\n",
    "<li>open cmd</li>\n",
    "<li><pre>snowsql -a < accountname >.< region > -u < username ></pre>\n",
    "    example\n",
    "    <pre>snowsql -a yta07909.us-east-1 -u SFADMIN</pre>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0973d",
   "metadata": {},
   "source": [
    "<h3>External Tables</h3>\n",
    "<ul>\n",
    "    <li>External Tables allow you to create tables on data stored external to Snowflake</li>\n",
    "    <ul>\n",
    "        <li>Definition in Snowflake metadata</li>\n",
    "        <li>Data in external location</li>\n",
    "    </ul>\n",
    "    <li>External Table functionality lets you query an external table like a regulat table</li>\n",
    "    <ul>\n",
    "        <li>Join with other tables</li>\n",
    "        <li>Create views on top of external tables</li>\n",
    "    </ul>\n",
    "    <li>External tables are read-only since they point to external storage</li>\n",
    "    <ul>\n",
    "        <li>DML operations cannot be performed on them</li>\n",
    "    </ul>\n",
    "    <li>External table data does not contribute to snowflake storage costs</li>\n",
    "    <ul>\n",
    "        <li>the compute cost is charged when queries are executed</li>\n",
    "    </ul>\n",
    "    <li>External tables point to an external stage</li>\n",
    "    <li>External table's definition contains column information and file format information</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5fb4a",
   "metadata": {},
   "source": [
    "<h2>Basic Data Transformation</h2>\n",
    "<h3>Basic transformation allowed during ingestion using COPY</h3>\n",
    "<ul>\n",
    "<li>Omit column or columns</li>\n",
    "<li>Change the order of columns</li>\n",
    "<li>Cast columns into specific data types</li>\n",
    "<li>Truncate (if the column value is larger than the desired length)</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Transformation not supported during COPY</h3>\n",
    "<ul>\n",
    "    <li>Joins</li>\n",
    "    <li>Filter i.e WHERE clause</li>\n",
    "    <li>Aggregation functions and Flatten </li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70747e3f",
   "metadata": {},
   "source": [
    "<h3>Unloading Data from Snowflake</h3>\n",
    "<ul>\n",
    "<li>Unloading data or exporting data is almost the same as loading</li>\n",
    "<li>It uses the same COPY mechanism and the concept of stages</li>\n",
    "<li>Unload to different file formats</li>\n",
    "<li>Compress & Encrypt while unloading</li>\n",
    "<li>Supported File Formats for data unloading</li>\n",
    "    <ul>\n",
    "        <li>Simple delimited files</li>\n",
    "            <ul>\n",
    "                <li>CSV</li>\n",
    "                <li>TSV</li>\n",
    "                <li>Any other separator character</li>\n",
    "            </ul>    \n",
    "        <li>Parquet</li>\n",
    "        <li>JSON</li>\n",
    "        <ul>\n",
    "            <li>Only NDJSON (new line delimited JSON) is supported</li>\n",
    "        </ul>    \n",
    "    </ul>    \n",
    " <li>Compression</li>\n",
    "    <ul>\n",
    "        <li>Automatically compressed to gzip</li>\n",
    "        <li>Can change compression mechanism</li>\n",
    "        <li>Can disable compression if required</li>\n",
    "    </ul>    \n",
    " <li>Encryption</li>\n",
    "    <ul>\n",
    "        <li>Automaticcally encrypted when exporting to an internal stage</li>\n",
    "        <li>Not automatically encrypted when exported to an external stage</li>\n",
    "        <ul>\n",
    "            <li>Although can be configured to encrypt by providing an encryption key</li>\n",
    "        </ul>    \n",
    "    </ul>    \n",
    " <li>Single vs Multiple files</li>\n",
    "    <ul>\n",
    "        <li>Exported to multiple files by default</li>\n",
    "        <li>Can be changed to a single file</li>\n",
    "    </ul>    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df961e0",
   "metadata": {},
   "source": [
    "<img width=\"863\" alt=\"image\" src=\"https://github.com/melwinmpk/AmazonBooks_DataPipeline/assets/25386607/6adcdb63-c03b-4777-8b1c-dde1b9fc5549\">\n",
    "<img width=\"863\" alt=\"image\" src=\"https://github.com/melwinmpk/AmazonBooks_DataPipeline/assets/25386607/62aee7fc-9a70-4b80-b010-25412983dcb8\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8cd5b05",
   "metadata": {},
   "source": [
    "<img width=\"890\" alt=\"image\" src=\"https://github.com/melwinmpk/AmazonBooks_DataPipeline/assets/25386607/a02671dc-4fd9-49c8-8b03-21b8adc13b2c\">\n",
    "<img width=\"898\" alt=\"image\" src=\"https://github.com/melwinmpk/AmazonBooks_DataPipeline/assets/25386607/f3e651db-c225-415b-8848-1f821c97c482\">\n",
    "<img width=\"810\" alt=\"image\" src=\"https://github.com/melwinmpk/AmazonBooks_DataPipeline/assets/25386607/df36490f-631f-45d0-8934-56e767f1aca9\">\n",
    "<img width=\"894\" alt=\"image\" src=\"https://github.com/melwinmpk/AmazonBooks_DataPipeline/assets/25386607/7b1687c7-fef5-47d3-8afe-d360f6d6d291\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f908a0",
   "metadata": {},
   "source": [
    "<p>In the Snowsight worksheet view, which of the following can be selected to set the context under which a query executes?</p>\n",
    "<ul>\n",
    "    <li>User</li>\n",
    "    <li><b>Role</b></li>\n",
    "    <li><b>Virtual Warehouse</b></li>\n",
    "    <li>Table</li>\n",
    "    <li><b>Schema</b></li>\n",
    "    <li><b>Database</b></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<p>Which columns will be part of the result set when a directory table is queried?</p>\n",
    "<ul>\n",
    "    <li>IS_ENCRYPTED</li>\n",
    "    <li><b>ETAG</b></li>\n",
    "    <li><b>MD5</b></li>\n",
    "    <li>IS_DELETED</li>\n",
    "    <li><b>LAST_MODIFIED</b></li>\n",
    "</ul>\n",
    "\n",
    "<p>Which of the following columns will be part of the result set when a directory table is queried?</p>\n",
    "<ul>\n",
    "<li><b>RELATIVE_PATH</b></li>\n",
    "<li>ENCRYPTION_KEY</li>\n",
    "<li><b>SIZE</b></li>\n",
    "<li><b>FILE_URL</b></li>\n",
    "<li>IS_COMPRESSED</li>\n",
    "</ul>\n",
    "\n",
    "<p>You are required to regularly refresh the metadata for a directory table for an external stage. Which of the following methods can you use?</p>\n",
    "<ul>\n",
    "<li><b>Configure even notifications on the cloud platform and set the stage to auto-refresh</b></li>\n",
    "<li>Request Snowflake support to refresh the stage periodically.</li>\n",
    "<li>Configure a Resource Monitor to refresh the stage periodically.</li>\n",
    "<li><b>Run ALTER STAGE &lt; stage-name &gt; REFRESH; periodically to manually refresh the metadata</b></li>\n",
    "</ul>\n",
    "\n",
    "<p>Which of the following is correct regarding a directory table?</p>\n",
    "<ul>\n",
    "<li><b>A directory table provides a catalog of files staged in a Stage object.</b></li>\n",
    "<li>Clustering keys can be defined on directory tables.</li>\n",
    "<li><b>You can query a directory table to obtain File URLs for each file in a Stage.</b></li>\n",
    "<li>Directory tables store CSV data in a VARIANT column.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Which of the following is correct regarding a directory table?</p>\n",
    "<ul>\n",
    "    <li>Privileges can be assigned to a directory table.</li>\n",
    "    <li><b>Privileges can NOT be assigned directly to a directory table.</b></li>\n",
    "    <li>A directory table is a separate object.</li>\n",
    "    <li><b>A directory table is NOT a separate object.</b></li>\n",
    "</ul>\n",
    "\n",
    "<p>Which of the following is true regarding Directory Tables?</p>\n",
    "<ul>\n",
    "    <li><b>Streams can be used with directory tables.</b></li>\n",
    "    <li>To use a stream with a directory table, you must create the stream on the directory table object.</li>\n",
    "    <li>Streams can NOT be used with directory tables</li>\n",
    "    <li><b>To use a stream with a directory table, you must create the stream on the stage object.</b></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<p>Which of the following actions occur when a directory table's metadata is refreshed?</p>\n",
    "<ul>\n",
    "<li><b>The directory table's metadata reflects changes to the file paths.</b></li>\n",
    "<li><b>Files that have been deleted are removed from the directory table's metadata.</b></li>\n",
    "<li><b>Files that are new in the external stage are added to the directory table's metadata.</b></li>\n",
    "</ul>    \n",
    "\n",
    "<p>An external stage can be enabled during the creation of a stage object or enabled afterward. Which of the following SQL enables the directory table?</p>\n",
    "<ul>\n",
    "<li>CREATE DIRECTORY TABLE < TABLE_NAME > ON < STAGE_NAME >;</li>\n",
    "<li>ALTER STAGE &lt;STAGE_NAME&gt; ADD DIRECTORY;</li>\n",
    "<li><b>ALTER STAGE < STAGE_NAME > SET DIRECTORY = (ENABLE = TRUE);</b></li>\n",
    "<li>GENERATE DIRECTORY TABLE ON < STAGE_NAME >;</li>    \n",
    "</ul>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c2df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
